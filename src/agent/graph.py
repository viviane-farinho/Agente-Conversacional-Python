"""
Agente Secret√°ria IA usando LangGraph
"""
from typing import TypedDict, Annotated, Sequence
from datetime import datetime
import operator

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

from src.config import Config
from src.agent.tools import ALL_TOOLS, set_context
from src.agent.prompts import get_system_prompt, TEXT_FORMAT_PROMPT
from src.services.database import get_db_service


class AgentState(TypedDict):
    """Estado do agente"""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    phone: str
    account_id: str
    conversation_id: str
    message_id: str
    telegram_chat_id: str
    is_audio_message: bool


class SecretaryAgent:
    """Agente Secret√°ria IA com LangGraph"""

    def __init__(self, model_provider: str = "openrouter"):
        """
        Inicializa o agente

        Args:
            model_provider: Provedor do modelo ("openrouter", "google" ou "openai")
        """
        self.model_provider = model_provider
        self.tools = ALL_TOOLS
        self.graph = self._build_graph()

    def _get_llm(self):
        """Retorna o modelo LLM configurado"""
        if self.model_provider == "openrouter":
            # OpenRouter usa API compat√≠vel com OpenAI
            return ChatOpenAI(
                model=Config.OPENROUTER_MODEL,
                api_key=Config.OPENROUTER_API_KEY,
                base_url="https://openrouter.ai/api/v1",
                temperature=0.7,
                default_headers={
                    "HTTP-Referer": "https://github.com/secretaria-ia",
                    "X-Title": "Secretaria IA"
                }
            )
        elif self.model_provider == "google":
            return ChatGoogleGenerativeAI(
                model="gemini-2.0-flash",
                google_api_key=Config.GOOGLE_API_KEY,
                temperature=0.7
            )
        else:
            return ChatOpenAI(
                model="gpt-4o",
                api_key=Config.OPENAI_API_KEY,
                temperature=0.7
            )

    def _build_graph(self) -> StateGraph:
        """Constr√≥i o grafo do agente"""

        # LLM com ferramentas
        llm = self._get_llm()
        llm_with_tools = llm.bind_tools(self.tools)

        async def agent_node(state: AgentState) -> dict:
            """N√≥ do agente que processa mensagens"""
            messages = state["messages"]

            # Log das ferramentas dispon√≠veis
            print(f"üõ†Ô∏è Ferramentas dispon√≠veis: {[t.name for t in self.tools]}")

            response = await llm_with_tools.ainvoke(messages)

            # Log detalhado da resposta
            print(f"üì§ Resposta do LLM:")
            print(f"   - Tipo: {type(response)}")
            print(f"   - Conte√∫do: {response.content[:200] if response.content else 'vazio'}...")
            if hasattr(response, "tool_calls"):
                print(f"   - Tool calls: {response.tool_calls}")
            else:
                print(f"   - Tool calls: NENHUM (atributo n√£o existe)")
            if hasattr(response, "additional_kwargs"):
                print(f"   - additional_kwargs: {response.additional_kwargs}")

            return {"messages": [response]}

        def should_continue(state: AgentState) -> str:
            """Decide se deve continuar para ferramentas ou finalizar"""
            last_message = state["messages"][-1]

            # Se a √∫ltima mensagem tem tool_calls, vai para as ferramentas
            if hasattr(last_message, "tool_calls") and last_message.tool_calls:
                return "tools"

            return END

        async def tools_node(state: AgentState) -> dict:
            """N√≥ de ferramentas com logging"""
            last_message = state["messages"][-1]
            tool_calls = last_message.tool_calls if hasattr(last_message, "tool_calls") else []

            print(f"üîß Ferramentas chamadas: {[tc['name'] for tc in tool_calls]}")
            for tc in tool_calls:
                print(f"   - {tc['name']}: {tc['args']}")

            # Usa o ToolNode padr√£o
            tool_node = ToolNode(self.tools)
            try:
                result = await tool_node.ainvoke(state)
                print(f"‚úÖ Resultado das ferramentas: {result}")
                return result
            except Exception as e:
                print(f"‚ùå Erro nas ferramentas: {e}")
                import traceback
                traceback.print_exc()
                raise

        # Constr√≥i o grafo
        workflow = StateGraph(AgentState)

        # Adiciona os n√≥s
        workflow.add_node("agent", agent_node)
        workflow.add_node("tools", tools_node)

        # Define o ponto de entrada
        workflow.set_entry_point("agent")

        # Adiciona as arestas condicionais
        workflow.add_conditional_edges(
            "agent",
            should_continue,
            {
                "tools": "tools",
                END: END
            }
        )

        # Ferramentas sempre voltam para o agente
        workflow.add_edge("tools", "agent")

        return workflow.compile()

    async def _load_history(self, phone: str) -> list[BaseMessage]:
        """Carrega o hist√≥rico de mensagens do banco de dados"""
        db = await get_db_service()
        history = await db.get_message_history(phone)

        messages = []
        for msg in history:
            if msg["role"] == "human":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))

        return messages

    async def _save_to_history(self, phone: str, role: str, content: str):
        """Salva uma mensagem no hist√≥rico"""
        db = await get_db_service()
        await db.add_message_to_history(phone, role, content)

    async def process_message(
        self,
        message: str,
        phone: str,
        account_id: str,
        conversation_id: str,
        message_id: str,
        telegram_chat_id: str,
        is_audio_message: bool = False
    ) -> str:
        """
        Processa uma mensagem do usu√°rio

        Args:
            message: Mensagem do usu√°rio
            phone: Telefone do usu√°rio
            account_id: ID da conta no Chatwoot
            conversation_id: ID da conversa
            message_id: ID da mensagem
            telegram_chat_id: ID do chat do Telegram para alertas
            is_audio_message: Se a mensagem original era um √°udio

        Returns:
            Resposta do agente
        """
        # Define o contexto para as ferramentas
        set_context(
            account_id=account_id,
            conversation_id=conversation_id,
            message_id=message_id,
            phone=phone,
            telegram_chat_id=telegram_chat_id
        )

        # Carrega o hist√≥rico
        history = await self._load_history(phone)

        # Monta o prompt do sistema
        system_prompt = get_system_prompt(phone, conversation_id)

        # Monta as mensagens
        messages = [
            SystemMessage(content=system_prompt),
            *history,
            HumanMessage(content=message)
        ]

        # Estado inicial
        initial_state = {
            "messages": messages,
            "phone": phone,
            "account_id": account_id,
            "conversation_id": conversation_id,
            "message_id": message_id,
            "telegram_chat_id": telegram_chat_id,
            "is_audio_message": is_audio_message
        }

        # Executa o grafo
        result = await self.graph.ainvoke(initial_state)

        # Obt√©m a resposta final
        last_message = result["messages"][-1]
        response = last_message.content if hasattr(last_message, "content") else str(last_message)

        # Salva no hist√≥rico
        await self._save_to_history(phone, "human", message)
        await self._save_to_history(phone, "assistant", response)

        return response

    async def format_response_for_whatsapp(self, text: str) -> str:
        """
        Formata a resposta para WhatsApp

        Args:
            text: Texto original

        Returns:
            Texto formatado
        """
        llm = self._get_llm()

        messages = [
            SystemMessage(content=TEXT_FORMAT_PROMPT),
            HumanMessage(content=text)
        ]

        response = await llm.ainvoke(messages)
        return response.content


# Inst√¢ncia global do agente
_agent = None


def get_agent(model_provider: str = "openrouter") -> SecretaryAgent:
    """Retorna a inst√¢ncia do agente (sempre recria para pegar atualizacoes)"""
    global _agent
    # Sempre recria o agente para pegar atualizacoes no prompt e ferramentas
    _agent = SecretaryAgent(model_provider=model_provider)
    return _agent
